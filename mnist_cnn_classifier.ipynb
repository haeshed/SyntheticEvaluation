{"cells":[{"cell_type":"code","execution_count":25,"metadata":{"id":"EhD5Io0VH5C2","trusted":true},"outputs":[],"source":["import os\n","import random\n","import shutil\n","import torch\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","from torch import nn\n","from torchvision import datasets, transforms\n","from torchvision.transforms import v2\n","from torch.utils.data import DataLoader, Subset\n","from torchvision.datasets import ImageFolder # that can be applied on these datasets\n","\n","# https://www.kaggle.com/code/vikasbhadoria/mnist-data-99-5-accuracy-using-pytorch/"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"7ZFjoVVOIR4j","trusted":true},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # To use to cuda GPU\n","\n","# data/classifier/\n","# ├── train/\n","# │   ├── class_0/  (contains images for class 0)\n","# │   ├── class_1/  (contains images for class 1)\n","# │   ├── .../\n","# └── val/\n","#     ├── class_0/\n","#     ├── class_1/\n","#     ├── .../\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing classes: 100%|██████████| 10/10 [00:00<00:00, 367.59it/s]"]},{"name":"stdout","output_type":"stream","text":["Files have been successfully organized into training and validation sets.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["input_dir = './data/mnist_images'  # This is the directory with all 60K .png images\n","train_dir = './data/classifier/train' # 10K\n","val_dir = './data/classifier/val' # 50K\n","\n","# Create directories if they don't exist\n","os.makedirs(train_dir, exist_ok=True)\n","os.makedirs(val_dir, exist_ok=True)\n","\n","for i in range(10):\n","    os.makedirs(os.path.join(train_dir, f'class_{i}'), exist_ok=True)\n","    os.makedirs(os.path.join(val_dir, f'class_{i}'), exist_ok=True)\n","\n","# Collect all .png files\n","png_files = [f for f in os.listdir(input_dir) if f.endswith('.png')]\n","\n","# Classify files by their label\n","class_files = {str(i): [] for i in range(10)}\n","for f in png_files:\n","    class_label = f.split('_')[0]  # Extract class from the filename (class_randomId.png)\n","    if class_label in class_files:\n","        class_files[class_label].append(f)\n","\n","# Progress tracking and batch size control\n","total_train_size = 10000  # Target for the training set\n","total_val_size = 50000    # Target for the validation set\n","train_size_per_class = total_train_size // 10  # Target train size per class (1000)\n","val_size_per_class = total_val_size // 10      # Target validation size per class (5000)\n","\n","# Set the deviation limit to 30 files\n","deviation_limit = 30\n","\n","# Process each class and ensure each has similar numbers of files\n","for class_label, files in tqdm(class_files.items(), desc=\"Processing classes\", total=10):\n","    random.shuffle(files)\n","\n","    # Split into training and validation based on the batch size rules\n","    class_total = len(files)\n","    val_count = min(val_size_per_class, class_total - train_size_per_class)\n","    train_count = class_total - val_count\n","\n","    # Ensure the deviation limit of ±30\n","    if abs(train_count - train_size_per_class) > deviation_limit:\n","        adjustment = (train_count - train_size_per_class) // abs(train_count - train_size_per_class)\n","        train_count = train_size_per_class + adjustment * deviation_limit\n","        val_count = class_total - train_count\n","\n","    # Separate files into validation and training sets\n","    val_files = files[:val_count]\n","    train_files = files[val_count:val_count + train_count]\n","\n","    # print(f\"\\nClass {class_label}: Train size {len(train_files)}, Val size {len(val_files)}\")\n","\n","    # Move validation files to the 'val' directory\n","    for file_name in tqdm(val_files, desc=f\"Moving validation files for class {class_label}\", leave=False):\n","        src_path = os.path.join(input_dir, file_name)\n","        dst_path = os.path.join(val_dir, f'class_{class_label}', file_name)\n","        shutil.move(src_path, dst_path)\n","    \n","    # Move training files to the 'train' directory\n","    for file_name in tqdm(train_files, desc=f\"Moving training files for class {class_label}\", leave=False):\n","        src_path = os.path.join(input_dir, file_name)\n","        dst_path = os.path.join(train_dir, f'class_{class_label}', file_name)\n","        shutil.move(src_path, dst_path)\n","\n","print(\"Files have been successfully organized into training and validation sets.\")\n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# Define your transformations (Resize, Tensor conversion, and Normalization)\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),\n","    # transforms.Grayscale(num_output_channels=1),  # In case the images are not grayscale\n","    v2.RGB(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","# Use ImageFolder to load images from local directories\n","training_dataset = ImageFolder(root='./data/classifier/train', transform=transform)\n","validation_dataset = ImageFolder(root='./data/classifier/val', transform=transform)\n","\n","val_dataset_size = len(validation_dataset)\n","# Create a subset by specifying indices\n","# You can select a fixed number or percentage of the dataset for the subset\n","subset_indices = np.random.choice(val_dataset_size, size=5000, replace=False)  # Subsample 5,000 images\n","validation_subset_dataset = Subset(validation_dataset, subset_indices)\n","\n","\n","\n","\n","# Create DataLoader for the training and validation datasets\n","training_loader = DataLoader(training_dataset, batch_size=100, shuffle=True)\n","# validation_loader = DataLoader(validation_dataset, batch_size=100, shuffle=False)\n","validation_loader = validation_subset_loader = DataLoader(validation_subset_dataset, batch_size=100, shuffle=True)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# len(training_dataset)\n","# len(validation_dataset)\n","# len(validation_subset_dataset)"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"jW15TfMiJ2sk","trusted":true},"outputs":[],"source":["class LeNet(nn.Module):\n","    def __init__(self):\n","      super().__init__()\n","      self.conv1 = nn.Conv2d(1, 20, 5, 1) # Conv layer1\n","      self.conv2 = nn.Conv2d(20, 50, 5, 1) # Conv layer2\n","      self.fc1 = nn.Linear(4*4*50, 500)    # Fully connected layer1\n","      self.dropout1 = nn.Dropout(0.5)   # We use dropout layer between these both FCL as they have the highest number of parameters b/t them\n","      self.fc2 = nn.Linear(500, 10)   # Fully connected layer2\n","    def forward(self, x):\n","      x = F.relu(self.conv1(x))  # Apply ReLu to the feature maps produced after Conv 1 layer\n","      x = F.max_pool2d(x, 2, 2)  # Pooling layer after Conv 1 layer\n","      x = F.relu(self.conv2(x))  # Apply ReLu to the feature maps produced after Conv 2 layer\n","      x = F.max_pool2d(x, 2, 2)  # Pooling layer after Conv 2 layer\n","      print(x.shape)\n","      batch_size = x.size(0)  # Get the batch size\n","      x = x.view(batch_size, -1)  # Flatten to [batch_size, 50 * 5 * 5 = 1250]\n","\n","      # x = x.view(-1, 4*4*50)     # Flattening the output of CNN to feed it into Fully connected layer\n","      x = F.relu(self.fc1(x))   # Fully connected layer 1 with Relu\n","      x = self.dropout1(x)     # We use dropout layer between these both FCL as they have the highest number of parameters b/t them\n","      x = self.fc2(x)         # Fully connected layer 2 with no activation funct as we need raw output from CrossEntropyLoss\n","      return x"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"KumHcA2gL8Po","outputId":"21bb9c62-c0e5-48b9-8a89-847f408e7b3f","trusted":true},"outputs":[{"data":{"text/plain":["LeNet(\n","  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=800, out_features=500, bias=True)\n","  (dropout1): Dropout(p=0.5, inplace=False)\n","  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",")"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["model = LeNet().to(device)\n","model"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"nD0Tcsu9Oqkn","trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"PCPw4kGMO3BK","outputId":"b1a1864f-b7c2-4b99-9e75-fb2caf50781c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([100, 50, 5, 5])\n"]},{"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (100x1250 and 800x500)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[41], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Put our inputs and labels in the device as our model is also in the device\u001b[39;00m\n\u001b[1;32m     16\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# giving input to our model to get corresponding output\u001b[39;00m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels) \u001b[38;5;66;03m# comparing out model's output to original labels\u001b[39;00m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m##sets the initial gradient to zero\u001b[39;00m\n","File \u001b[0;32m~/Documents/CodingProjects/SyntheticEvaluation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/CodingProjects/SyntheticEvaluation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[38], line 19\u001b[0m, in \u001b[0;36mLeNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten to [batch_size, 50 * 5 * 5 = 1250]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# x = x.view(-1, 4*4*50)     # Flattening the output of CNN to feed it into Fully connected layer\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)   \u001b[38;5;66;03m# Fully connected layer 1 with Relu\u001b[39;00m\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)     \u001b[38;5;66;03m# We use dropout layer between these both FCL as they have the highest number of parameters b/t them\u001b[39;00m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)         \u001b[38;5;66;03m# Fully connected layer 2 with no activation funct as we need raw output from CrossEntropyLoss\u001b[39;00m\n","File \u001b[0;32m~/Documents/CodingProjects/SyntheticEvaluation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/CodingProjects/SyntheticEvaluation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/CodingProjects/SyntheticEvaluation/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x1250 and 800x500)"]}],"source":["epochs = 15\n","running_loss_history = []\n","running_corrects_history = []\n","val_running_loss_history = []\n","val_running_corrects_history = []\n","\n","for e in range(epochs):\n","  \n","  running_loss = 0.0\n","  running_corrects = 0.0\n","  val_running_loss = 0.0\n","  val_running_corrects = 0.0\n","  \n","  for inputs, labels in training_loader: # As our train_loader is batch size of 100 and had input images and corresponding labels\n","    inputs = inputs.to(device)  # Put our inputs and labels in the device as our model is also in the device\n","    labels = labels.to(device)\n","    outputs = model(inputs)   # giving input to our model to get corresponding output\n","    loss = criterion(outputs, labels) # comparing out model's output to original labels\n","    \n","    optimizer.zero_grad()  ##sets the initial gradient to zero\n","    loss.backward()  ## The whole calculated loss is then back propogated to the model\n","    optimizer.step()  ## Then the weights are updated by doing their derivative w.r.t the Loss\n","    \n","    _, preds = torch.max(outputs, 1) # Then we select the max value of raw output and consider it as our prediction. We select it from 10 o/ps\n","    running_loss += loss.item()  # total loss of 1 epoch\n","    running_corrects += torch.sum(preds == labels.data) #total accuracy of 1 epoch\n","\n","  else:\n","    with torch.no_grad(): # This we done to set no gradient as we do not need it for val as our model is already trained.\n","      for val_inputs, val_labels in validation_loader:\n","        val_inputs = val_inputs.to(device)  # Put our val_inputs and labels in the device as our model is also in the device\n","        val_labels = val_labels.to(device)\n","        val_outputs = model(val_inputs)\n","        val_loss = criterion(val_outputs, val_labels)\n","        \n","        _, val_preds = torch.max(val_outputs, 1)\n","        val_running_loss += val_loss.item()\n","        val_running_corrects += torch.sum(val_preds == val_labels.data)\n","      \n","    epoch_loss = running_loss/len(training_loader)\n","    epoch_acc = running_corrects.float()/ len(training_loader)\n","    running_loss_history.append(epoch_loss)\n","    running_corrects_history.append(epoch_acc)\n","    \n","    val_epoch_loss = val_running_loss/len(validation_loader)\n","    val_epoch_acc = val_running_corrects.float()/ len(validation_loader)\n","    val_running_loss_history.append(val_epoch_loss)\n","    val_running_corrects_history.append(val_epoch_acc)\n","    print('epoch :', (e+1))\n","    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n","    print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc.item()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mu4RXFsIRo7j","outputId":"13a68ca6-28e0-4a95-b217-4999e1bf2159","trusted":true},"outputs":[],"source":["plt.plot(running_loss_history, label='training loss')\n","plt.plot(val_running_loss_history, label='validation loss')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrWonqhvTFt6","outputId":"74d29907-e3af-4b4e-8a39-c8a247f410b8","trusted":true},"outputs":[],"source":["plt.plot(running_corrects_history, label='training accuracy')\n","plt.plot(val_running_corrects_history, label='validation accuracy')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3R8ri2c8TRiJ","trusted":true},"outputs":[],"source":["import PIL.ImageOps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qm7CAooTWIt","outputId":"ec06ecb8-84ce-4e6d-b0ef-0d921b2c316a","trusted":true},"outputs":[],"source":["import requests\n","from PIL import Image\n","\n","url = 'https://images.homedepot-static.com/productImages/007164ea-d47e-4f66-8d8c-fd9f621984a2/svn/architectural-mailboxes-house-letters-numbers-3585b-5-64_1000.jpg'\n","response = requests.get(url, stream = True)\n","img = Image.open(response.raw)\n","plt.imshow(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYjqL1WWTYzT","outputId":"303d374c-c0db-4e30-ad22-9f0dff6f2974","trusted":true},"outputs":[],"source":["img = PIL.ImageOps.invert(img)  # we use Image operations from PIL to invert(i.e. make white black and vice versa)\n","img = img.convert('1') # we convert from RGB to Gray\n","img = transform(img) # Apply the transform funct we defined earlier to make our downloaded img same as what we trained on\n","plt.imshow(im_convert(img))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxSHTxFYTr4I","outputId":"fe3313b8-2e1b-4b95-c8d2-699bd6fd2d4d","trusted":true},"outputs":[],"source":["images = img.to(device)  # As our model is in the device\n","image = images[0].unsqueeze(0).unsqueeze(0)\n","output = model(image)\n","_, pred = torch.max(output, 1)\n","print(pred.item())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UiHW9DmUB-x","outputId":"fd0eb2b7-db1b-49dc-c5ae-9eb43be38aeb","trusted":true},"outputs":[],"source":["dataiter = iter(validation_loader)\n","images, labels = dataiter.next()\n","images = images.to(device)\n","labels = labels.to(device)\n","output = model(images)\n","_, preds = torch.max(output, 1)\n","\n","fig = plt.figure(figsize=(25, 4))\n","\n","for idx in np.arange(20):\n","  ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n","  plt.imshow(im_convert(images[idx]))\n","  ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())), color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":861823,"sourceId":3004,"sourceType":"competition"}],"dockerImageVersionId":29955,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
