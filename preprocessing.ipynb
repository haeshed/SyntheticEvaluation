{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.v2\n",
    "from PIL import Image\n",
    "import ipynbname\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_home = \"/Users/hadare/Documents/CodingProjects/SyntheticEvaluation\"\n",
    "path_img =  path_home + \"/images\"\n",
    "path_dataset = path_home + \"/dataset\"\n",
    "path_exp = path_home + \"/experiments\"\n",
    "raw_data = path_home + \"/data/mnist_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:04<00:00, 2465210.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 187442.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:04<00:00, 349873.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 1126184.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create directories to save MNIST images\n",
    "mnist_dir = raw_data\n",
    "os.makedirs(mnist_dir, exist_ok=True)\n",
    "\n",
    "# Download MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    # transforms.Grayscale(num_output_channels=1),  # In case the images are not grayscale\n",
    "    transforms.v2.RGB(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "mnist_dataset = datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Save each image as a separate file\n",
    "for idx, (img, label) in enumerate(mnist_dataset):\n",
    "    img_path = os.path.join(mnist_dir, f'{label}_{idx}.png')\n",
    "    img = transforms.ToPILImage()(img)\n",
    "    img.save(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35904f15b7a41e1884fc798a2512866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing files:   0%|          | 0/60000 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 12000/60000 files...\n",
      "Processed 24000/60000 files...\n",
      "Processed 36000/60000 files...\n",
      "Processed 48000/60000 files...\n",
      "Processed 60000/60000 files...\n",
      "\n",
      "Processing complete. Summary:\n",
      "Total files processed: 60000\n",
      "Consistent files: 60000\n",
      "Files with inconsistent size: 0\n",
      "Files with inconsistent color format: 0\n",
      "Base size: (32, 32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test all files in path_img for size and format:\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "from PIL import Image   \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "IMAGE_PATH = raw_data\n",
    "files = [f for f in listdir(IMAGE_PATH) if isfile(join(IMAGE_PATH, f))]\n",
    "\n",
    "base_size = None\n",
    "consistent_files = 0\n",
    "inconsistent_size_files = 0\n",
    "inconsistent_color_files = 0\n",
    "total_files = len(files)\n",
    "\n",
    "# Set the interval for status updates (e.g., every 100 files)\n",
    "update_interval = int(total_files/5)\n",
    "\n",
    "# Create a progress bar\n",
    "progress_bar = tqdm(total=total_files, desc=\"Processing files\", unit=\"file\")\n",
    "\n",
    "for i, file in enumerate(files, 1):\n",
    "    file_path = os.path.join(IMAGE_PATH, file)\n",
    "    img = Image.open(file_path)\n",
    "    sz = img.size\n",
    "    \n",
    "    if base_size is None:\n",
    "        base_size = sz\n",
    "    \n",
    "    if sz != base_size:\n",
    "        inconsistent_size_files += 1\n",
    "    elif img.mode != 'RGB':\n",
    "        inconsistent_color_files += 1\n",
    "    else:\n",
    "        consistent_files += 1\n",
    "    \n",
    "    # Update the progress bar\n",
    "    progress_bar.update(1)\n",
    "    \n",
    "    # Print status update every x files\n",
    "    if i % update_interval == 0:\n",
    "        print(f\"Processed {i}/{total_files} files...\")\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Print completion stats\n",
    "print(\"\\nProcessing complete. Summary:\")\n",
    "print(f\"Total files processed: {total_files}\")\n",
    "print(f\"Consistent files: {consistent_files}\")\n",
    "print(f\"Files with inconsistent size: {inconsistent_size_files}\")\n",
    "print(f\"Files with inconsistent color format: {inconsistent_color_files}\")\n",
    "print(f\"Base size: {base_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide into subdirectories\n",
    "\n",
    "def distribute_files(src_dir):\n",
    "    # Create class directories if they don't exist\n",
    "    for class_num in range(10):\n",
    "        class_dir = os.path.join(src_dir, f\"class_{class_num}\")\n",
    "        if not os.path.exists(class_dir):\n",
    "            os.makedirs(class_dir)\n",
    "\n",
    "    # Distribute files into their respective class directories\n",
    "    for filename in os.listdir(src_dir):\n",
    "        src_path = os.path.join(src_dir, filename)\n",
    "        if os.path.isfile(src_path):\n",
    "            try:\n",
    "                file_class = filename.split(\"_\")[0][0]\n",
    "                if file_class.isdigit():\n",
    "                    dst_path = os.path.join(src_dir, f\"class_{file_class}\", filename)\n",
    "                    shutil.move(src_path, dst_path)\n",
    "            except (IndexError, ValueError):\n",
    "                # Handle files that don't follow the expected naming convention\n",
    "                print(f\"Skipping file: {filename} (doesn't follow the expected naming convention)\")\n",
    "\n",
    "source_directory = raw_data\n",
    "distribute_files(source_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format for the dataset.json file:\n",
    "\n",
    "# {\n",
    "#     \"labels\":\n",
    "#         [\n",
    "#             [\"folder1/1.jpg\", 0], [\"folder1/2.jpg\", 0], [\"folder1/3.jpg\", 0], \n",
    "#             [\"folder2/4.jpg\", 1], [\"folder2/5.jpg\", 1], [\"folder2/6.jpg\", 1], \n",
    "#             [\"folder3/7.jpg\", 2], [\"folder3/8.jpg\", 2], [\"folder3/9.jpg\", 2], \n",
    "#         ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hadare/Documents/CodingProjects/SyntheticEvaluation/data/mnist_images\n"
     ]
    }
   ],
   "source": [
    "def generate_labels_json(base_dir,output_dir, output_file_name):\n",
    "    labels_data = {\"labels\": []}\n",
    "    \n",
    "    # Walk through the base directory and its subdirectories\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            # Only include image files (you can adjust this based on file extensions)\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')):\n",
    "                # Extract the class label from the first digit of the file name\n",
    "                try:\n",
    "                    label = int(file[0])  # First digit of the file name\n",
    "                except ValueError:\n",
    "                    # Skip files that do not start with a digit\n",
    "                    continue\n",
    "                \n",
    "                # Create relative file path from the base directory\n",
    "                file_path = os.path.relpath(os.path.join(root, file), base_dir)\n",
    "                \n",
    "                # Add the file path and corresponding label to the list\n",
    "                labels_data[\"labels\"].append([file_path, label])\n",
    "\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "    # Write the labels to the output JSON file\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        json.dump(labels_data, f, indent=4)\n",
    "\n",
    "base_directory = raw_data\n",
    "print(base_directory)\n",
    "generate_labels_json(base_directory, base_directory, \"dataset.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def load_data(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data['labels']\n",
    "\n",
    "def create_dataframe(data):\n",
    "    df = pd.DataFrame(data, columns=['file_path', 'label'])\n",
    "    return df\n",
    "\n",
    "def subset_data(df, subset_size, seed=None):\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "\n",
    "    class_counts = Counter(df['label'])\n",
    "    num_classes = len(class_counts)\n",
    "    class_subset_sizes = {class_id: int(subset_size / num_classes) for class_id in class_counts}\n",
    "\n",
    "    subsets = []\n",
    "    for class_id, count in class_counts.items():\n",
    "        class_df = df[df['label'] == class_id]\n",
    "        class_subset = class_df.sample(n=class_subset_sizes[class_id], replace=False, random_state=seed)\n",
    "        subsets.append(class_subset)\n",
    "\n",
    "    subset_df = pd.concat(subsets, ignore_index=True)\n",
    "    return subset_df\n",
    "\n",
    "def save_data(subset_df, output_file):\n",
    "    data = {\n",
    "        \"labels\": subset_df.values.tolist()\n",
    "    }\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "input_file = raw_data + \"/dataset.json\"\n",
    "output_file = raw_data + \"/dataset_subset.json\"\n",
    "subset_size = 1000\n",
    "seed_value = 42\n",
    "\n",
    "data = load_data(input_file)\n",
    "df = create_dataframe(data)\n",
    "subset_df = subset_data(df, subset_size, seed=seed_value)\n",
    "save_data(subset_df, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/hadare/Documents/CodingProjects/SyntheticEvaluation/data/mnist_images/dataset.json\n",
      "Splitting data into train and test sets with train ratio 0.8 and seed 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data: 100%|██████████| 10/10 [00:00<00:00, 255.96class/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train data to /Users/hadare/Documents/CodingProjects/SyntheticEvaluation/data/mnist_images/train_data.json\n",
      "Saving test data to /Users/hadare/Documents/CodingProjects/SyntheticEvaluation/data/mnist_images/test_data.json\n",
      "Train data statistics:\n",
      "label\n",
      "0    4738\n",
      "1    5393\n",
      "2    4766\n",
      "3    4904\n",
      "4    4673\n",
      "5    4336\n",
      "6    4734\n",
      "7    5012\n",
      "8    4680\n",
      "9    4759\n",
      "Name: count, dtype: int64\n",
      "Test data statistics:\n",
      "label\n",
      "0    1185\n",
      "1    1349\n",
      "2    1192\n",
      "3    1227\n",
      "4    1169\n",
      "5    1085\n",
      "6    1184\n",
      "7    1253\n",
      "8    1171\n",
      "9    1190\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data['labels']\n",
    "\n",
    "def create_dataframe(data):\n",
    "    df = pd.DataFrame(data, columns=['file_path', 'label'])\n",
    "    return df\n",
    "\n",
    "def split_data(df, train_ratio, seed=None):\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    grouped = df.groupby('label')\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "\n",
    "    for _, group in tqdm(grouped, desc='Splitting data', unit='class'):\n",
    "        group = group.sample(frac=1, random_state=seed).reset_index(drop=True)  # Shuffle the group\n",
    "        train_size = int(len(group) * train_ratio)\n",
    "        train_data.extend(group.iloc[:train_size].to_dict('records'))\n",
    "        test_data.extend(group.iloc[train_size:].to_dict('records'))\n",
    "\n",
    "    train_df = pd.DataFrame(train_data, columns=['file_path', 'label'])\n",
    "    test_df = pd.DataFrame(test_data, columns=['file_path', 'label'])\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def save_data(data_df, output_file):\n",
    "    data = {\n",
    "        \"labels\": data_df.values.tolist()\n",
    "    }\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "input_file = raw_data + \"/dataset.json\"\n",
    "# output_file = raw_data + \"/dataset_subset.json\"\n",
    "output_dir = raw_data\n",
    "\n",
    "# input_file = 'path/to/input.json'\n",
    "# output_dir = 'path/to/output_dir'\n",
    "train_ratio = 0.8  # 80% for train, 20% for test\n",
    "seed_value = 42\n",
    "\n",
    "print(f\"Loading data from {input_file}\")\n",
    "data = load_data(input_file)\n",
    "df = create_dataframe(data)\n",
    "\n",
    "print(f\"Splitting data into train and test sets with train ratio {train_ratio} and seed {seed_value}\")\n",
    "train_df, test_df = split_data(df, train_ratio, seed=seed_value)\n",
    "\n",
    "train_output_file = f\"{output_dir}/train_data.json\"\n",
    "test_output_file = f\"{output_dir}/test_data.json\"\n",
    "\n",
    "print(f\"Saving train data to {train_output_file}\")\n",
    "save_data(train_df, train_output_file)\n",
    "\n",
    "print(f\"Saving test data to {test_output_file}\")\n",
    "save_data(test_df, test_output_file)\n",
    "\n",
    "print(\"Train data statistics:\")\n",
    "print(train_df['label'].value_counts().sort_index())\n",
    "\n",
    "print(\"Test data statistics:\")\n",
    "print(test_df['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/hadare/Documents/CodingProjects/SyntheticEvaluation/data/mnist_images/dataset.json\n",
      "Subsetting data with size 4000 and seed 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subsetting classes: 100%|██████████| 10/10 [00:00<00:00, 1630.00class/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to /Users/hadare/Documents/CodingProjects/SyntheticEvaluation/data/mnist_images/dataset_subset_size4000_seed42.json\n",
      "Subset statistics:\n",
      "label\n",
      "0    400\n",
      "1    400\n",
      "2    400\n",
      "3    400\n",
      "4    400\n",
      "5    400\n",
      "6    400\n",
      "7    400\n",
      "8    400\n",
      "9    400\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data['labels']\n",
    "\n",
    "def create_dataframe(data):\n",
    "    df = pd.DataFrame(data, columns=['file_path', 'label'])\n",
    "    return df\n",
    "\n",
    "def subset_data(df, subset_size, seed=None):\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "\n",
    "    class_counts = Counter(df['label'])\n",
    "    num_classes = len(class_counts)\n",
    "    class_subset_sizes = {class_id: int(subset_size / num_classes) for class_id in class_counts}\n",
    "\n",
    "    subsets = []\n",
    "    for class_id, count in tqdm(class_counts.items(), desc='Subsetting classes', unit='class'):\n",
    "        class_df = df[df['label'] == class_id]\n",
    "        class_subset = class_df.sample(n=class_subset_sizes[class_id], replace=False, random_state=seed)\n",
    "        subsets.append(class_subset)\n",
    "\n",
    "    subset_df = pd.concat(subsets, ignore_index=True)\n",
    "    return subset_df\n",
    "\n",
    "def save_data(subset_df, output_file):\n",
    "    data = {\n",
    "        \"labels\": subset_df.values.tolist()\n",
    "    }\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "subset_size = 4000\n",
    "seed_value = 42\n",
    "input_file = raw_data + \"/dataset.json\"\n",
    "# output_file = raw_data + \"/dataset_subset.json\"\n",
    "output_file = raw_data + f\"/dataset_subset_size{subset_size}_seed{seed_value}.json\"\n",
    "\n",
    "print(f\"Loading data from {input_file}\")\n",
    "data = load_data(input_file)\n",
    "df = create_dataframe(data)\n",
    "\n",
    "print(f\"Subsetting data with size {subset_size} and seed {seed_value}\")\n",
    "subset_df = subset_data(df, subset_size, seed=seed_value)\n",
    "\n",
    "print(f\"Saving data to {output_file}\")\n",
    "save_data(subset_df, output_file)\n",
    "\n",
    "print(\"Subset statistics:\")\n",
    "print(subset_df['label'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
