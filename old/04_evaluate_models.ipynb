{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from setup import load_test_data\n",
    "from utils import load_model, calculate_metrics\n",
    "\n",
    "# Load and evaluate the model on real test data\n",
    "def evaluate_model(model_name, test_data):\n",
    "    model = load_model(model_name)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_data:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test images: {accuracy}%')\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_synthetic_performance():\n",
    "    real_test_data = load_test_data()\n",
    "    real_model_score = evaluate_model('real_model.pth', real_test_data)\n",
    "\n",
    "    # Compare with synthetic models\n",
    "    synthetic_model_scores = []\n",
    "    for size in [100, 500, 1000]:\n",
    "        synthetic_model_scores.append(evaluate_model(f'synthetic_{size}.pth', real_test_data))\n",
    "\n",
    "    return real_model_score, synthetic_model_scores\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
