{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import .py files\n",
    "import os\n",
    "import pandas as pd\n",
    "# import train_model\n",
    "# import generate_synthetic\n",
    "# import train_classifier\n",
    "# import test_imgs\n",
    "import split_dataset_v2 as split\n",
    "import train_model as train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_home = \"/Users/hadare/Documents/CodingProjects/SyntheticEvaluation\"\n",
    "path_img =  path_home + \"/images\"\n",
    "path_dataset = path_home + \"/dataset\"\n",
    "path_exp = path_home + \"/experiments\"\n",
    "path_models = path_home + \"/models\"\n",
    "raw_data = path_home + \"/data/mnist_images\"\n",
    "\n",
    "# Set the initial directory for experiments\n",
    "initial_experiment_dir = \"./experiments\"\n",
    "os.makedirs(initial_experiment_dir, exist_ok=True)  # Create directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "Train Size: [1000, 2000, 3000]\n",
      "Generation Size: [500, 1000, 1500]\n",
      "Synthetic/Real Ratio: 0.5\n",
      "StyleGAN2-ADA Training Cutoff: 5000\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# set parameters for which train_size, gen_size, synthetic/real ratio, GAN train cutoff\n",
    "# set initial dir for experiments\n",
    "\n",
    "# Set parameters as lists of integer values\n",
    "seed = 42\n",
    "train_sizes = [1000, 2000, 3000]  # List of different training sizes\n",
    "gen_sizes = [500, 1000, 1500]      # List of different generation sizes\n",
    "synthetic_real_ratio = 0.5          # Ratio of synthetic to real data\n",
    "gan_train_cutoff = 5000             # Number of GAN training iterations before switching\n",
    "train_ratio = 0.8\n",
    "\n",
    "\n",
    "# Print out the configured parameters for verification\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"Train Size: {train_sizes}\")\n",
    "print(f\"Generation Size: {gen_sizes}\")\n",
    "print(f\"Synthetic/Real Ratio: {synthetic_real_ratio}\")\n",
    "print(f\"StyleGAN2-ADA Training Cutoff: {gan_train_cutoff}\")\n",
    "# print(f\"Experiment Run Directory: {experiment_run_dir}\")\n",
    "print(\"-\" * 40)  # Separator for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use preprocessing.ipynb to create a proper dataset\n",
    "# Distribute files to relevant subfolders + create JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data: 100%|██████████| 10/10 [00:00<00:00, 269.21class/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data statistics:\n",
      "label\n",
      "0    4738\n",
      "1    5393\n",
      "2    4766\n",
      "3    4904\n",
      "4    4673\n",
      "5    4336\n",
      "6    4734\n",
      "7    5012\n",
      "8    4680\n",
      "9    4759\n",
      "Name: count, dtype: int64\n",
      "Test data statistics:\n",
      "label\n",
      "0    1185\n",
      "1    1349\n",
      "2    1192\n",
      "3    1227\n",
      "4    1169\n",
      "5    1085\n",
      "6    1184\n",
      "7    1253\n",
      "8    1171\n",
      "9    1190\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# split into train/test\n",
    "\n",
    "full_json = raw_data + '/dataset.json'\n",
    "\n",
    "# split into test/train subsets\n",
    "input_file = full_json\n",
    "\n",
    "\n",
    "\n",
    "train_df, test_df = split.split_train_test(input_file, train_ratio, seed=seed)\n",
    "\n",
    "train_output_file = f\"{raw_data}/train_data.json\"\n",
    "test_output_file = f\"{raw_data}/test_data.json\"\n",
    "\n",
    "split.save_data(train_df, train_output_file)\n",
    "split.save_data(test_df, test_output_file)\n",
    "\n",
    "split.print_class_distribution(train_df, \"Train\")\n",
    "split.print_class_distribution(test_df, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP/CNN classifier, test for benchmark using test_imgs.py\n",
    "# V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subsetting classes: 100%|██████████| 10/10 [00:00<00:00, 1846.33class/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset data statistics:\n",
      "label\n",
      "0    100\n",
      "1    100\n",
      "2    100\n",
      "3    100\n",
      "4    100\n",
      "5    100\n",
      "6    100\n",
      "7    100\n",
      "8    100\n",
      "9    100\n",
      "Name: count, dtype: int64\n",
      "Training model1K with 1000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for loop: create dir, create dataset, train model, generate syn-images, test\n",
    "\n",
    "input_file = raw_data + '/train_data.json'\n",
    "\n",
    "train_sizes = [1000]\n",
    "for subset_size in train_sizes:\n",
    "    # Create a name for the subset and the model\n",
    "    model_name = f\"model{subset_size // 1000}K\"\n",
    "    # subset the data according to size\n",
    "    model_dir = path_models + '/' + model_name\n",
    "    experiments_dir = os.path.join(model_dir, 'experiments')\n",
    "    dataset_dir = os.path.join(model_dir, 'dataset')\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    os.makedirs(experiments_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    subset_df = split.subset_data(input_file, subset_size, seed=seed)\n",
    "    output_file = f\"{model_dir}/dataset_subset_size{subset_size}_seed{seed}.json\"\n",
    "    split.save_data(subset_df, output_file)\n",
    "    split.print_class_distribution(subset_df, \"Subset\")\n",
    "    \n",
    "    \n",
    "    print(f\"Training {model_name} with {subset_size} images...\")\n",
    "    train.run_stylegan_training(subset_dir, experiments_dir, path_home, snap=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results - relevant metric: avg/class accuracy, f1, precision, recall, AUC-ROC...\n",
    "# graph/tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete datasets (keep logs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
