{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import .py files\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import json\n",
    "# import train_model\n",
    "# import generate_synthetic\n",
    "# import train_classifier\n",
    "# import test_imgs\n",
    "import split_dataset as split\n",
    "import train_model\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_home = '/home/pathorad3090/Documents/Hadar/SyntheticEvaluation'\n",
    "path_models = path_home + \"/models\"\n",
    "path_raw_data = path_home + \"/data/mnist_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set parameters for which train_size, gen_size, synthetic/real ratio, GAN train cutoff\n",
    "# Set initial dir for experiments\n",
    "\n",
    "# Set parameters as lists of integer values\n",
    "seed = 42\n",
    "train_sizes = [50, 100, 400, 800, 1000, 5000, 10000]  # List of different training sizes\n",
    "gen_sizes = [30]  # List of different generation sizes\n",
    "synthetic_real_ratio = 0.5  # Ratio of synthetic to real data\n",
    "gan_train_cutoff = 5000  # Number of GAN training iterations before switching\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Log the configured parameters for verification\n",
    "logger.info(\"Training Configuration:\")\n",
    "logger.info(f\"Train Sizes: {train_sizes}\")\n",
    "logger.info(f\"Generation Sizes: {gen_sizes}\")\n",
    "logger.info(f\"Synthetic/Real Ratio: {synthetic_real_ratio}\")\n",
    "logger.info(f\"StyleGAN2-ADA Training Cutoff: {gan_train_cutoff}\")\n",
    "logger.info(f\"Train Ratio: {train_ratio}\")\n",
    "logger.info(\"-\" * 40)  # Separator for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use preprocessing.ipynb to create a proper dataset\n",
    "# Distribute files to relevant subfolders + create JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define paths and parameters\n",
    "full_json = path_home + '/data/dataset_full.json'\n",
    "train_json = full_json\n",
    "\n",
    "logger.info(f\"Loading data from {full_json} for splitting into train/test subsets.\")\n",
    "\n",
    "# Split into train/test subsets\n",
    "train_df, test_df = split.split_train_test(train_json, train_ratio, seed=seed)\n",
    "\n",
    "train_output_file = f\"{path_raw_data}/train_data.json\"\n",
    "test_output_file = f\"{path_raw_data}/test_data.json\"\n",
    "\n",
    "logger.info(f\"Saving training data to {train_output_file}.\")\n",
    "split.save_data(train_df, train_output_file)\n",
    "\n",
    "logger.info(f\"Saving testing data to {test_output_file}.\")\n",
    "split.save_data(test_df, test_output_file)\n",
    "\n",
    "logger.info(\"Printing class distribution for training and testing datasets.\")\n",
    "split.print_class_distribution(train_df, \"Train\")\n",
    "split.print_class_distribution(test_df, \"Test\")\n",
    "\n",
    "logger.info(\"Data splitting and saving completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP/CNN classifier, test for benchmark using test_imgs.py\n",
    "# V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export MKL_SERVICE_FORCE_INTEL=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from random import seed\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "train_json = path_raw_data + '/train_data.json'\n",
    "\n",
    "# Example of train_sizes; ensure it's defined somewhere\n",
    "train_sizes = [40000]  # Adjust this as needed\n",
    "\n",
    "for train_size in train_sizes:\n",
    "    # Create a name for the subset and the model\n",
    "    model_name = f\"model_{float(train_size) // 1000}K_{datetime.now().strftime('%Y%m%d_%H-%M')}\"\n",
    "    path_model = os.path.join(path_models, model_name)\n",
    "    path_experiments = os.path.join(path_model, 'experiments')\n",
    "    path_dataset = os.path.join(path_model, 'dataset')\n",
    "    path_subset_json = f\"{path_model}/dataset_subset_size_{train_size}_seed_{seed}.json\"\n",
    "    \n",
    "    logger.info(f\"Creating directories for model: {model_name}\")\n",
    "    _, _, path_model_images = split.open_folders(model_name, path_model)\n",
    "    \n",
    "    logger.info(f\"Generating subset data from {train_json} with size {train_size} and seed {seed}\")\n",
    "    subset_df = split.subset_data(train_json, train_size, seed)\n",
    "    \n",
    "    logger.info(f\"Subset DataFrame created. Path raw data: {path_raw_data}, Path model images: {path_model_images}\")\n",
    "    split.copy_images_to_model_and_dataset(subset_df, path_raw_data, path_model_images)\n",
    "    \n",
    "    logger.info(f\"Saving subset data to {path_subset_json}\")\n",
    "    split.save_data(subset_df, path_subset_json)\n",
    "\n",
    "    logger.info(f\"Distributing files into label directories at {path_model_images}\")\n",
    "    split.distribute_files_to_label_dirs(path_model_images)\n",
    "    \n",
    "    logger.info(\"Generating labels JSON for the subset.\")\n",
    "    split.generate_labels_json(path_model_images, path_model_images, \"dataset.json\")\n",
    "    \n",
    "    logger.info(f\"Creating dataset for {model_name}...\")\n",
    "    train_model.create_dataset(path_home, path_model_images, path_dataset)\n",
    "\n",
    "    logger.info(f\"Training model {model_name}...\")\n",
    "    train_model.run_stylegan_training(path_home, path_experiments, path_dataset, snap=10)\n",
    "    \n",
    "    logger.info(f\"Cleaning up model directories for {model_name}\")\n",
    "    split.delete_images_and_dataset_dirs(path_model)\n",
    "    \n",
    "    path_latest_pkl_file = split.get_latest_pkl_file(path_model)\n",
    "    if path_latest_pkl_file:\n",
    "        logger.info(f\"Most recent .pkl file: {path_latest_pkl_file}\")\n",
    "        for gen_size in gen_sizes:\n",
    "            gen_size = gen_size // 10\n",
    "            path_generations = os.path.join(path_model, 'generations_', str(gen_size * 10))\n",
    "            logger.info(f\"Generating synthetic images for {model_name} with generation size {gen_size * 10}...\")\n",
    "            train_model.generate_stylegan_images(path_home, path_latest_pkl_file, path_generations, f\"0-{gen_size}\")\n",
    "    else:\n",
    "        logger.warning(\"No .pkl file found for generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results - relevant metric: avg/class accuracy, f1, precision, recall, AUC-ROC...\n",
    "# graph/tabular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete datasets (keep logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def open_image_folder(source_dir, *, max_images=100000):\n",
    "#     input_images = [str(f) for f in sorted(Path(source_dir).rglob('*')) if is_image_ext(f) and os.path.isfile(f)]\n",
    "\n",
    "#     # Load labels.\n",
    "#     labels = {}\n",
    "#     meta_fname = os.path.join(source_dir, 'dataset.json')\n",
    "#     if os.path.isfile(meta_fname):\n",
    "#         with open(meta_fname, 'r') as file:\n",
    "#             labels = json.load(file)['labels']\n",
    "#             if labels is not None:\n",
    "#                 labels = { x[0]: x[1] for x in labels }\n",
    "#             else:\n",
    "#                 labels = {}\n",
    "\n",
    "\n",
    "#     def iterate_images():\n",
    "#         for idx, fname in enumerate(input_images):\n",
    "#             arch_fname = os.path.relpath(fname, source_dir)\n",
    "#             arch_fname = arch_fname.replace('\\\\', '/')\n",
    "#             img = np.array(PIL.Image.open(fname))\n",
    "#             yield dict(img=img, label=labels.get(arch_fname))\n",
    "#             if idx >= max_idx-1:\n",
    "#                 break\n",
    "#     return max_idx, iterate_images()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
