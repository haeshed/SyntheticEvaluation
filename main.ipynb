{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /path/to/input.json\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/input.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# import train_model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# import generate_synthetic\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# import train_classifier\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# import test_imgs\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msplit_dataset_v2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msplit\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/CodingProjects/SyntheticEvaluation/split_dataset_v2.py:124\u001b[0m\n\u001b[1;32m    118\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/path/to/output_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# For train/test split\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# main(input_file, output_dir, train_ratio=0.8, seed=42)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# For subsetting\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/CodingProjects/SyntheticEvaluation/split_dataset_v2.py:90\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(input_file, output_dir, train_ratio, subset_size, seed)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03mMain function to handle data splitting and subsetting.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_json_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m df \u001b[38;5;241m=\u001b[39m create_dataframe(data)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subset_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/CodingProjects/SyntheticEvaluation/split_dataset_v2.py:12\u001b[0m, in \u001b[0;36mload_json_data\u001b[0;34m(json_file)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json_data\u001b[39m(json_file):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    Load data from a JSON file with the structure {'labels': [...]}\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    Returns a list of [file_path, label] pairs.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     13\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/input.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "# import .py files\n",
    "import os\n",
    "import pandas as pd\n",
    "# import train_model\n",
    "# import generate_synthetic\n",
    "# import train_classifier\n",
    "# import test_imgs\n",
    "import split_dataset_v2 as split\n",
    "import train_model as train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_home = \"/Users/hadare/Documents/CodingProjects/SyntheticEvaluation\"\n",
    "path_img =  path_home + \"/images\"\n",
    "path_dataset = path_home + \"/dataset\"\n",
    "path_exp = path_home + \"/experiments\"\n",
    "path_models = path_home + \"/models\"\n",
    "raw_data = path_home + \"/data/mnist_images\"\n",
    "\n",
    "# Set the initial directory for experiments\n",
    "initial_experiment_dir = \"./experiments\"\n",
    "os.makedirs(initial_experiment_dir, exist_ok=True)  # Create directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for which train_size, gen_size, synthetic/real ratio, GAN train cutoff\n",
    "# set initial dir for experiments\n",
    "\n",
    "# Set parameters as lists of integer values\n",
    "seed = 42\n",
    "train_sizes = [1000, 2000, 3000]  # List of different training sizes\n",
    "gen_sizes = [500, 1000, 1500]      # List of different generation sizes\n",
    "synthetic_real_ratio = 0.5          # Ratio of synthetic to real data\n",
    "gan_train_cutoff = 5000             # Number of GAN training iterations before switching\n",
    "\n",
    "\n",
    "# Print out the configured parameters for verification\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"Train Size: {train_size}\")\n",
    "print(f\"Generation Size: {gen_size}\")\n",
    "print(f\"Synthetic/Real Ratio: {synthetic_real_ratio}\")\n",
    "print(f\"GANG Training Cutoff: {gan_train_cutoff}\")\n",
    "print(f\"Experiment Run Directory: {experiment_run_dir}\")\n",
    "print(\"-\" * 40)  # Separator for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use preprocessing.ipynb to create a proper dataset\n",
    "# Distribute files to relevant subfolders + create JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "\n",
    "full_json = raw_data + 'dataset.json'\n",
    "\n",
    "# split into test/train subsets\n",
    "input_file = \"/path/to/input.json\"\n",
    "model_dir = \"/path/to/output_dir\"\n",
    "train_ratio = 0.8\n",
    "seed = 42\n",
    "\n",
    "train_df, test_df = split.split_train_test(input_file, train_ratio, seed=seed)\n",
    "\n",
    "train_output_file = f\"{model_dir}/train_data.json\"\n",
    "test_output_file = f\"{model_dir}/test_data.json\"\n",
    "\n",
    "split.save_data(train_df, train_output_file)\n",
    "split.save_data(test_df, test_output_file)\n",
    "\n",
    "split.print_class_distribution(train_df, \"Train\")\n",
    "split.print_class_distribution(test_df, \"Test\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP/CNN classifier, test for benchmark using test_imgs.py\n",
    "# V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop: create dir, create dataset, train model, generate syn-images, test\n",
    "\n",
    "input_file = \"/path/to/input.json\"\n",
    "\n",
    "for subset_size in train_sizes:\n",
    "    # Create a name for the subset and the model\n",
    "    model_name = f\"model{subset_size // 1000}K\"\n",
    "    # subset the data according to size\n",
    "    model_dir = path_models + '/' + model_name\n",
    "    subset_size = subset_size\n",
    "\n",
    "    subset_df = split.subset_data(input_file, subset_size, seed=seed)\n",
    "    output_file = f\"{model_dir}/dataset_subset_size{subset_size}_seed{seed}.json\"\n",
    "    split.save_data(subset_df, output_file)\n",
    "    split.print_class_distribution(subset_df, \"Subset\")\n",
    "    \n",
    "    experiments_dir = os.path.join(model_dir, 'experiments')\n",
    "    dataset_dir = os.path.join(model_dir, 'dataset')\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    os.makedirs(experiments_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f\"Training {model_name} with {subset_size} images...\")\n",
    "    train.run_stylegan_training(subset_dir, model_save_path, path_home, snap=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results - relevant metric: avg/class accuracy, f1, precision, recall, AUC-ROC...\n",
    "# graph/tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete datasets (keep logs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
