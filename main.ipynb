{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import .py files\n",
    "import os\n",
    "import pandas as pd\n",
    "# import train_model\n",
    "# import generate_synthetic\n",
    "# import train_classifier\n",
    "# import test_imgs\n",
    "import split_dataset as split\n",
    "import train_model as train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_home = \"/Users/hadare/Documents/CodingProjects/SyntheticEvaluation\"\n",
    "path_img =  path_home + \"/images\"\n",
    "path_dataset = path_home + \"/dataset\"\n",
    "path_exp = path_home + \"/experiments\"\n",
    "path_models = path_home + \"/models\"\n",
    "raw_data = path_home + \"/data/mnist_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "Train Size: [1000, 2000, 3000]\n",
      "Generation Size: [500, 1000, 1500]\n",
      "Synthetic/Real Ratio: 0.5\n",
      "StyleGAN2-ADA Training Cutoff: 5000\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# set parameters for which train_size, gen_size, synthetic/real ratio, GAN train cutoff\n",
    "# set initial dir for experiments\n",
    "\n",
    "# Set parameters as lists of integer values\n",
    "seed = 42\n",
    "train_sizes = [1000, 2000, 3000]  # List of different training sizes\n",
    "gen_sizes = [500, 1000, 1500]      # List of different generation sizes\n",
    "synthetic_real_ratio = 0.5          # Ratio of synthetic to real data\n",
    "gan_train_cutoff = 5000             # Number of GAN training iterations before switching\n",
    "train_ratio = 0.8\n",
    "\n",
    "\n",
    "# Print out the configured parameters for verification\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"Train Size: {train_sizes}\")\n",
    "print(f\"Generation Size: {gen_sizes}\")\n",
    "print(f\"Synthetic/Real Ratio: {synthetic_real_ratio}\")\n",
    "print(f\"StyleGAN2-ADA Training Cutoff: {gan_train_cutoff}\")\n",
    "# print(f\"Experiment Run Directory: {experiment_run_dir}\")\n",
    "print(\"-\" * 40)  # Separator for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use preprocessing.ipynb to create a proper dataset\n",
    "# Distribute files to relevant subfolders + create JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data: 100%|██████████| 10/10 [00:00<00:00, 166.25class/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data statistics:\n",
      "Total number of samples: 47995\n",
      "label\n",
      "0    4738\n",
      "1    5393\n",
      "2    4766\n",
      "3    4904\n",
      "4    4673\n",
      "5    4336\n",
      "6    4734\n",
      "7    5012\n",
      "8    4680\n",
      "9    4759\n",
      "Name: count, dtype: int64\n",
      "Test data statistics:\n",
      "Total number of samples: 12005\n",
      "label\n",
      "0    1185\n",
      "1    1349\n",
      "2    1192\n",
      "3    1227\n",
      "4    1169\n",
      "5    1085\n",
      "6    1184\n",
      "7    1253\n",
      "8    1171\n",
      "9    1190\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# split into train/test\n",
    "\n",
    "full_json = path_home + '/data/dataset_full.json'\n",
    "\n",
    "# split into test/train subsets\n",
    "input_file = full_json\n",
    "\n",
    "\n",
    "\n",
    "train_df, test_df = split.split_train_test(input_file, train_ratio, seed=seed)\n",
    "\n",
    "train_output_file = f\"{raw_data}/train_data.json\"\n",
    "test_output_file = f\"{raw_data}/test_data.json\"\n",
    "\n",
    "split.save_data(train_df, train_output_file)\n",
    "split.save_data(test_df, test_output_file)\n",
    "\n",
    "split.print_class_distribution(train_df, \"Train\")\n",
    "split.print_class_distribution(test_df, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP/CNN classifier, test for benchmark using test_imgs.py\n",
    "# V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subsetting classes: 100%|██████████| 10/10 [00:00<00:00, 2269.03class/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset for model_1K...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'train_model' has no attribute 'create_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# split.print_class_distribution(subset_df, \"Subset\")\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating dataset for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m(path_home, model_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dataset\u001b[39m\u001b[38;5;124m'\u001b[39m, raw_data)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# print(path_home, experiments_dir, dataset_dir)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(f\"Training {model_name}...\")\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# train.run_stylegan_training(path_home, experiments_dir, dataset_dir, snap=10)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'train_model' has no attribute 'create_dataset'"
     ]
    }
   ],
   "source": [
    "# for loop: create dir, create dataset, train model, generate syn-images, test\n",
    "\n",
    "input_file = raw_data + '/train_data.json'\n",
    "\n",
    "train_sizes = [1000]\n",
    "for subset_size in train_sizes:\n",
    "    # Create a name for the subset and the model\n",
    "    model_name = f\"model_{subset_size // 1000}K\"\n",
    "    # subset the data according to size\n",
    "    model_dir = path_models + '/' + model_name\n",
    "    experiments_dir = os.path.join(model_dir, 'experiments')\n",
    "    dataset_dir = os.path.join(model_dir, 'dataset')\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    os.makedirs(experiments_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    subset_df = split.subset_data(input_file, subset_size, seed=seed)\n",
    "    subset_json_path = f\"{model_dir}/dataset_subset_size_{subset_size}_seed_{seed}.json\"\n",
    "    split.save_data(subset_df, subset_json_path)\n",
    "    split.save_data(subset_df, raw_data + \"/dataset.json\")\n",
    "    # split.print_class_distribution(subset_df, \"Subset\")\n",
    "    \n",
    "    print(f\"Creating dataset for {model_name}...\")\n",
    "    train.create_dataset(path_home, model_dir + '/dataset', raw_data)\n",
    "\n",
    "    # print(path_home, experiments_dir, dataset_dir)\n",
    "    # print(f\"Training {model_name}...\")\n",
    "    # train.run_stylegan_training(path_home, experiments_dir, dataset_dir, snap=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results - relevant metric: avg/class accuracy, f1, precision, recall, AUC-ROC...\n",
    "# graph/tabular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete datasets (keep logs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
